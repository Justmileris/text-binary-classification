{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TextBinaryClassification.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Justmileris/TextBinaryClassification/blob/master/TextBinaryClassification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "evEuqqZAHfW4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "93dn4ytkIZKG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from chardet import detect\n",
        "import os\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import pos_tag\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from collections import defaultdict\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "from nltk.corpus import wordnet as wn\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn import model_selection, naive_bayes, svm\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "import re\n",
        "\n",
        "import spacy\n",
        "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
        "from sklearn.base import TransformerMixin\n",
        "from sklearn.pipeline import Pipeline\n",
        "import string\n",
        "from spacy.lang.en.stop_words import STOP_WORDS\n",
        "from spacy.lang.en import English\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import metrics\n",
        "\n",
        "import keras\n",
        "from keras import regularizers\n",
        "from keras.layers import Dropout\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.models import Sequential\n",
        "from keras import layers\n",
        "\n",
        "import time"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a-TgHYBrIpjr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# os.getcwd()\n",
        "%cd /content/gdrive/My Drive/TextAnalysis"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y2J5uH_FIsub",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "os.listdir()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h4o1C63rIuNa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.random.seed(42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kYRUlztcIzkr",
        "colab_type": "text"
      },
      "source": [
        "# Read data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "srxW2GvSIvur",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "filepath_pos = 'rt-polarity.pos'\n",
        "filepath_neg = 'rt-polarity.neg'\n",
        "\n",
        "def get_encoding(file):\n",
        "    with open(file, 'rb') as f:\n",
        "        rawdata = f.read()\n",
        "    return detect(rawdata)['encoding']\n",
        "\n",
        "data_pos = pd.read_csv('rt-polarity.pos', sep=\"\\n\", header=None, encoding = get_encoding(filepath_pos), names=['Sentence'])\n",
        "data_neg = pd.read_csv('rt-polarity.neg', sep=\"\\n\", header=None, encoding = get_encoding(filepath_neg), names=['Sentence'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TTmCAS7DI1_7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(len(data_pos))\n",
        "print(data_pos.head())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mXZYkskOI3rD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(len(data_neg))\n",
        "print(data_neg.head())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OIXX2Ty6I-f5",
        "colab_type": "text"
      },
      "source": [
        "# Labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qiNqCd8TI7gw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Add True/False label column\n",
        "data_pos['Label'] = 1\n",
        "data_neg['Label'] = 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4hxlNijBJAr6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Combined data\n",
        "whole_data = pd.concat([data_pos, data_neg])\n",
        "whole_data = whole_data.reset_index(drop=True)\n",
        "print(whole_data.keys())\n",
        "print(len(whole_data))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tsvRwi8VJBzD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(whole_data.info())\n",
        "print()\n",
        "print(whole_data.Label.value_counts())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rLHg3FexJE7N",
        "colab_type": "text"
      },
      "source": [
        "# Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2n-P8pImJDNX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Corpus will be broken into set of words\n",
        "whole_data['Splitted_sentence'] = [word_tokenize(sentence) for sentence in whole_data['Sentence']]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BNDnIxOqJHnz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(type(whole_data))\n",
        "print(len(whole_data))\n",
        "print(whole_data['Splitted_sentence'][0])\n",
        "print(whole_data['Splitted_sentence'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JVLq_-7fJL4g",
        "colab_type": "text"
      },
      "source": [
        "# Lemmatisation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u4teaXjmJKTL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# WordNetLemmatizer requires Pos tags to understand if the word is noun or verb or adjective etc. By default it is set to Noun\n",
        "tag_map = defaultdict(lambda : wn.NOUN)\n",
        "tag_map['J'] = wn.ADJ\n",
        "tag_map['V'] = wn.VERB\n",
        "tag_map['R'] = wn.ADV\n",
        "for index, sentence in enumerate(whole_data['Splitted_sentence']):\n",
        "    # Declaring Empty List to store the words that follow the rules for this step\n",
        "    Final_words = []\n",
        "    # Initializing WordNetLemmatizer()\n",
        "    word_Lemmatized = WordNetLemmatizer()\n",
        "    # pos_tag function below will provide the 'tag' i.e if the word is Noun(N) or Verb(V) or something else.\n",
        "    for word, tag in pos_tag(sentence):\n",
        "        # if word not in stopwords.words('english') and word.isalpha():\n",
        "        word_Final = word_Lemmatized.lemmatize(word,tag_map[tag[0]])\n",
        "        Final_words.append(word_Final)\n",
        "    whole_data.loc[index,'Tokens'] = str(Final_words)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-UsjX4k-JQMS",
        "colab_type": "text"
      },
      "source": [
        "# Data split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pc5EsbD5JPbj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Train_X, Test_X, Train_Y, Test_Y = model_selection.train_test_split(whole_data['Tokens'],whole_data['Label'],test_size=0.3, random_state=42, shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MwZfAK-ZJUJo",
        "colab_type": "text"
      },
      "source": [
        "# Vectorization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XIMhH_wtJSqb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create our list of punctuation marks\n",
        "punctuations = string.punctuation\n",
        "\n",
        "# Create our list of stopwords\n",
        "nlp = spacy.load('en')\n",
        "stop_words = spacy.lang.en.stop_words.STOP_WORDS\n",
        "\n",
        "# Load English tokenizer, tagger, parser, NER and word vectors\n",
        "parser = English()\n",
        "\n",
        "# Creating our tokenizer function\n",
        "def spacy_tokenizer(sentence):\n",
        "    # Creating our token object, which is used to create documents with linguistic annotations.\n",
        "    mytokens = parser(sentence)\n",
        "\n",
        "    # Lemmatizing each token and converting each token into lowercase\n",
        "    mytokens = [ word.lemma_.lower().strip() if word.lemma_ != \"-PRON-\" else word.lower_ for word in mytokens ]\n",
        "\n",
        "    # Removing stop words\n",
        "    # mytokens = [ word for word in mytokens if word not in stop_words and word not in punctuations ]\n",
        "    mytokens = [ word for word in mytokens if word not in punctuations ]\n",
        "\n",
        "    # return preprocessed list of tokens\n",
        "    return mytokens"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uJayuYjGJXD6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# vectorizer = TfidfVectorizer(tokenizer = spacy_tokenizer, ngram_range=(1,2), min_df=1, max_df=1.0) # 77.71\n",
        "vectorizer = CountVectorizer(tokenizer = spacy_tokenizer, ngram_range=(1,2), min_df=1, max_df=1.0) # 78.37\n",
        "\n",
        "vectorizer.fit(whole_data['Sentence'])\n",
        "\n",
        "Train_X_vectorized = vectorizer.transform(Train_X)\n",
        "Test_X_vectorized = vectorizer.transform(Test_X)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q-tYc5gBJaCo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(len(vectorizer.vocabulary_))\n",
        "print(vectorizer.vocabulary_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gA8PTCSWJdFl",
        "colab_type": "text"
      },
      "source": [
        "# Naive Bayes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OehkvmHmJbys",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Naive Bayes classifier\n",
        "classifier = naive_bayes.MultinomialNB()\n",
        "classifier.fit(Train_X_vectorized, Train_Y)\n",
        "prediction_NB = classifier.predict(Test_X_vectorized)\n",
        "print(\"Naive Bayes Accuracy Score: {}%\".format(round(accuracy_score(prediction_NB, Test_Y)*100, 2)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "18zoBnZnJjSY",
        "colab_type": "text"
      },
      "source": [
        "# SVM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "78CPdpFpJgoA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# SVM classifier\n",
        "classifier = svm.SVC(C=1.0, kernel='linear', degree=3, gamma='auto')\n",
        "classifier.fit(Train_X_vectorized,Train_Y)\n",
        "prediction_SVM = classifier.predict(Test_X_vectorized)\n",
        "print(\"SVM Accuracy Score: {}%\".format(round(accuracy_score(prediction_SVM, Test_Y)*100, 2)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KcmxQw2aJo4m",
        "colab_type": "text"
      },
      "source": [
        "# Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WQo_vOtlJmD0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Logistic Regression classifier\n",
        "classifier = LogisticRegression(solver='lbfgs', max_iter=2000)\n",
        "classifier.fit(Train_X_vectorized,Train_Y)\n",
        "prediction_logreg = classifier.predict(Test_X_vectorized)\n",
        "print(\"Logistic Regression Accuracy Score: {}%\".format(round(accuracy_score(prediction_logreg, Test_Y)*100, 2)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Re9FeAe3JtbG",
        "colab_type": "text"
      },
      "source": [
        "# Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z4EYpD6CJq_j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Random Forest\n",
        "classifier = RandomForestClassifier(n_estimators=10, random_state=42, verbose=3) # Add verbose=3 (more than 1) to see progress\n",
        "classifier.fit(Train_X_vectorized,Train_Y)\n",
        "prediction_randomforest = classifier.predict(Test_X_vectorized)\n",
        "print(\"Random Forest Accuracy Score: {}%\".format(round(accuracy_score(prediction_randomforest, Test_Y)*100, 2)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "77u3cX1vJy0W",
        "colab_type": "text"
      },
      "source": [
        "# Accuracies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nHntxKf_JwFr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"Naive Bayes: {}%\".format(round(accuracy_score(prediction_NB, Test_Y)*100, 2)))\n",
        "print(confusion_matrix(Test_Y, prediction_NB))\n",
        "print()\n",
        "print(\"SVM: {}%\".format(round(accuracy_score(prediction_SVM, Test_Y)*100, 2)))\n",
        "print(\"Logistic Regression: {}%\".format(round(accuracy_score(prediction_logreg, Test_Y)*100, 2)))\n",
        "print(\"Random Forest: {}%\".format(round(accuracy_score(prediction_randomforest, Test_Y)*100, 2)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yPTeOO5HJ5XK",
        "colab_type": "text"
      },
      "source": [
        "# Neural Networks, Keras ||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f1JY1ZjlJ02F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = Train_X_vectorized\n",
        "X_test = Test_X_vectorized\n",
        "y_train = Train_Y\n",
        "y_test = Test_Y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "thJ09mWEJ9ey",
        "colab_type": "text"
      },
      "source": [
        "Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kabu3gppJ8fd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.style.use('ggplot')\n",
        "epochs_count = 20\n",
        "\n",
        "input_dim = X_train.shape[1]  # Number of features\n",
        "model = Sequential()\n",
        "\n",
        "\n",
        "model.add(layers.Dense(4, input_dim=input_dim, activation='relu', kernel_regularizer=regularizers.l2(0.01)))\n",
        "# model.add(Dropout(0.1))\n",
        "model.add(layers.Dense(1, activation='sigmoid'))\n",
        "\n",
        "\n",
        "model.compile(loss='binary_crossentropy', \n",
        "              optimizer='adam', \n",
        "              metrics=['accuracy'])\n",
        "model.summary()\n",
        "\n",
        "history = model.fit(X_train, y_train,\n",
        "                    epochs=epochs_count,\n",
        "                    verbose=True,\n",
        "                    validation_data=(X_test, y_test),\n",
        "                    batch_size=32)\n",
        "\n",
        "\n",
        "def plot_history(history):\n",
        "    acc = history.history['acc']\n",
        "    val_acc = history.history['val_acc']\n",
        "    loss = history.history['loss']\n",
        "    val_loss = history.history['val_loss']\n",
        "    x = range(1, len(acc) + 1)\n",
        "\n",
        "    plt.figure(figsize=(12, 5))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(x, acc, 'b', label='Training acc')\n",
        "    plt.plot(x, val_acc, 'r', label='Validation acc')\n",
        "    plt.title('Training and validation accuracy')\n",
        "    plt.legend()\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(x, loss, 'b', label='Training loss')\n",
        "    plt.plot(x, val_loss, 'r', label='Validation loss')\n",
        "    plt.title('Training and validation loss')\n",
        "    plt.legend()\n",
        "\n",
        "\n",
        "loss, accuracy = model.evaluate(X_train, y_train, verbose=False)\n",
        "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
        "loss, accuracy = model.evaluate(X_test, y_test, verbose=False)\n",
        "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))\n",
        "plot_history(history)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_8u1-rPEKBoC",
        "colab_type": "text"
      },
      "source": [
        "Save model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5A0DWyAzJ_d8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This saves evrything (weight values, model's configuration(architecture), optimizer configuration)\n",
        "model_file_name = 'acc_' + str(int(round(accuracy*100))) + '_____epchs_' + str(epochs_count) + '_____' + str(int(time.time())) + '.h5'\n",
        "print(model_file_name)\n",
        "model.save(model_file_name)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kTdGS9iOKFz6",
        "colab_type": "text"
      },
      "source": [
        "Upload model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "179gaqP7KETb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Recreate the exact same model, including weights and optimizer.\n",
        "new_model = keras.models.load_model('acc_76_____epchs_20_____1572482736.h5')\n",
        "new_model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bkrhx3tgKIW8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loss, acc = new_model.evaluate(X_test, y_test)\n",
        "print(\"Restored model, accuracy: {:5.2f}%\".format(100*acc))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xw5zGdPhKQq9",
        "colab_type": "text"
      },
      "source": [
        "# EXTRA |||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UVhUo2dDKT_i",
        "colab_type": "text"
      },
      "source": [
        "# spaCy visualizations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1X-jQ2E_KNAo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Token word attributes\n",
        "\n",
        "print(whole_data['Sentence'][1])\n",
        "print()\n",
        "\n",
        "doc = nlp(whole_data['Sentence'][1])\n",
        "print('text\\tidx\\tlemma\\tis_pnct\\tis_spc\\tshape\\tpos\\ttag')\n",
        "for token in doc:\n",
        "    print(\"{0}\\t{1}\\t{2}\\t{3}\\t{4}\\t{5}\\t{6}\\t{7}\".format(\n",
        "        token.text,\n",
        "        token.idx,\n",
        "        token.lemma_,\n",
        "        token.is_punct,\n",
        "        token.is_space,\n",
        "        token.shape_,\n",
        "        token.pos_,\n",
        "        token.tag_\n",
        "    ))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9CdVzq7iKXEp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Entitites\n",
        "\n",
        "from spacy import displacy\n",
        " \n",
        "print(whole_data['Sentence'][2005])\n",
        "doc = nlp(whole_data['Sentence'][2005]) # 2005, 3496, 7533\n",
        "displacy.render(doc, style='ent', jupyter=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h6TZ48aMKZ2B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Dependencies\n",
        "\n",
        "from spacy import displacy\n",
        "index=0\n",
        "print(whole_data['Sentence'][index])\n",
        "doc = nlp(whole_data['Sentence'][index])\n",
        "\n",
        "displacy.render(doc, style='dep', jupyter=True, options={'distance': 100})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CQhCS6TWKbks",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word = whole_data['Sentence'][0]\n",
        "print(word)\n",
        "\n",
        "nlp(word).vector"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8mgrnaCrKdx7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}